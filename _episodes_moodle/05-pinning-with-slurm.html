<h1 id="thread-pinning-and-distribution-in-slurm">Thread pinning and distribution in Slurm</h1>
<h2 id="intended-learning-outcomes">Intended learning outcomes</h2>
<p>By the end of this lesson, you should be able to:</p>
<ul>
<li>Describe thread pinning and distribution and why they’re useful in modern HPC systems</li>
<li>Understand how pinning and distribution are used to enhance performance</li>
<li>Analyse and understand the hardware hierarchy of a particular system</li>
<li>Use Slurm to pin MPI processes to particular CPU sockets or cores</li>
</ul>
<h2 id="what-is-thread-pinning">What is thread pinning?</h2>
<p><em>Pinning</em>, also known as <em>affinity</em> or <em>binding</em><a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> is a feature of multicore systems where the active threads or processes of an application are <em>pinned</em> to certain physical processors or cores for the entire runtime of the application. This is useful because each physical CPU core in a HPC system does not have equal access to all resources. Some systems have two or more CPUs in a single node, so some regions of main memory are better connected to one CPU and have faster access times as a result. It’s common for systems to have one CPU but multiple NUMA zones: regions of main memory with Non-Uniform Memory Access. In these systems, groups of cores have faster access to certain parts of main memory. The memory is still technically accessible by all cores, but a core accessing memory in a region not associated with it will incur some extra access time.</p>
<p>To illustrate the performance impact of pinning, imagine a node with two sockets, i.e. two distinct physical CPUs. Each CPU has fast access to a local bank of main memory but slower access to the memory local to the other CPU. If a thread on this system isn’t pinned, that is the system is allowed to migrate threads between CPUs during runtime, it could initially allocate and use some space on the fast memory associated with its initial CPU, then be migrated across to the other CPU, where accessing the same block of memory will be significantly slower. By pinning this thread to a single CPU, or better a single core, we avoid this potentially slow memory access.</p>
<p>Of course, there will still be circumstances where a thread must access data in the memory bank that <em>isn’t</em> closest to its pinned CPU. Pinning is not a guarantee of fast memory access; it merely stops threads being unexpectedly migrated away from their local, fast memory.</p>
<div class="Challenge">
<p>Can you convince yourself that a system with one CPU but multiple NUMA zones could suffer from the same performance issues if threads were not pinned and allowed to migrate between NUMA zones?</p>
</div>
<h2 id="other-uses-of-thread-pinning">Other uses of thread pinning</h2>
<p>Memory is not the only resource in a HPC system that might be accessed in an unequal way by different cores. PCIe slots can also offer faster access to certain cores or processors. Since these slots can host performance-critical components like GPUs or network cards, pinning can ensure threads access the same resources consistently and optimally over the entire runtime of the application.</p>
<p>Again, to illustrate the performance impact of pinning, imagine the same two-CPU system as we considered previously but now each CPU has a GPU associated with it. That is, each CPU has fast access to its closest GPU, but slower access to GPUs associated with the other CPU. A thread might initialise on, say, CPU 0<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>, allocate some space on its local memory bank, and transfer some data from there to the associated GPU 0. This memory transfer will be as fast as possible. Without pinning, this same thread could be migrated to CPU 1. It would then slowly access the same memory space on CPU 0’s memory bank, but even worse, it may move data to GPU 0. Some systems may be able to notice that the memory transfer can take a faster path without going through CPU 1, but the worst-case scenario is the data is transferred between CPUs twice: once into CPU 1 then back again on its way to GPU 0.</p>
<h2 id="understanding-system-details-with-hwloc-ls">Understanding system details with <code>hwloc-ls</code></h2>
<p>It’s helpful to understand the details of the specific system we want to target, particularly to understand the number of cores available and how they are distributed between sockets and NUMA zones. The command <code>hwloc-ls</code> gives us this information. It should be available on all DiRAC systems but if the specific system or node you’re using doesn’t offer it, you may be able to use <code>nproc</code> and <code>numactl -s</code> as alternatives.</p>
<p>Here’s a Slurm script that we can use to print out the details of the CPU(s) on our worker node with <code>hwloc-ls</code>:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true"></a><span class="co">#!/usr/bin/env bash</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true"></a><span class="co">#SBATCH --nodes=1</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true"></a><span class="co">#SBATCH -o hwloc-ls.out</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true"></a><span class="co">#SBATCH --exclusive</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true"></a><span class="co">#SBATCH -p cosma8</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true"></a><span class="co">#SBATCH -t 00:10:00</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true"></a><span class="co">#SBATCH -A &lt;PROJECT_CODE&gt;</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true"></a><span class="ex">hwloc-ls</span></span></code></pre></div>
<p>You should copy this into a script named something like <code>hwloc-ls.sh</code> and submit it with <code>sbatch hwloc-ls.sh</code><a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>. Once this runs, you should have an output file <code>hwloc-ls.out</code> with contents similar to our example output below. The output can seem overwhelming so let’s go through the example in pieces.</p>
<p>The tool organises its output to represent the hardware hierarchy of the entire node. It’s slightly easier to understand this if we have an expectation of what the hardware actually is so I’ll spoil the surprise and summarise the hardware on this specific node. This node has two CPUs, each with 64 cores, grouped into NUMA zones of 16 cores each, for a total of 8 NUMA zones. Within each group of 16 cores, two subgroups of 8 cores share a single L3 cache, so there are two L3 caches per NUMA zone. Some groups of cores are directly attached to PCIe slots, with some of those connected to video or network cards.</p>
<p>Here, the output has been annotated with comments to describe each level, and some longer sections have been edited out and replaced with <code>...</code>.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true"></a><span class="ex">Machine</span> (1007GB total)  # <span class="ex">The</span> entire node</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true"></a>  <span class="ex">Package</span> L#0  # A single socket or CPU. Notice there is a Package L#1 listed lower down, because this system has two CPU sockets.</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true"></a>    <span class="ex">Group0</span> L#0  # A group of cores within one CPU that has a number of resources directly linked to it.</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true"></a>      <span class="ex">NUMANode</span> L#0 (P#0 125GB)  # <span class="ex">The</span> region of memory directly linked to this collection of cores</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true"></a>      <span class="ex">L3</span> L#0 (32MB)  # <span class="ex">The</span> L3 cache available to just 8 cores in this group.</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true"></a>        <span class="ex">L2</span> L#0 (512KB) <span class="ex">+</span> L1d L#0 (32KB) <span class="ex">+</span> L1i L#0 (32KB) <span class="ex">+</span> Core L#0  # The collection of caches associated with a single *physical* core</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true"></a>          <span class="ex">PU</span> L#0 (P#0)  # <span class="ex">Logical</span> core number 1</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true"></a>          <span class="ex">PU</span> L#1 (P#128)  # <span class="ex">And</span> logical core number 2</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true"></a>        <span class="ex">L2</span> L#1 (512KB) <span class="ex">+</span> L1d L#1 (32KB) <span class="ex">+</span> L1i L#1 (32KB) <span class="ex">+</span> Core L#1  # 2nd physical core</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true"></a>          <span class="ex">PU</span> L#2 (P#1)</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true"></a>          <span class="ex">PU</span> L#3 (P#129)</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true"></a></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true"></a>        <span class="ex">...</span> # Similar details of cores 2-6</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true"></a></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true"></a>        <span class="ex">L2</span> L#7 (512KB) <span class="ex">+</span> L1d L#7 (32KB) <span class="ex">+</span> L1i L#7 (32KB) <span class="ex">+</span> Core L#7  # The 8th core in this L3 cache group</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true"></a>          <span class="ex">PU</span> L#14 (P#7)</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true"></a>          <span class="ex">PU</span> L#15 (P#135)</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true"></a>      <span class="ex">L3</span> L#1 (32MB)  # <span class="ex">The</span> other L3 cache in this group, serving the other 8 cores</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true"></a>        <span class="ex">...</span> # The other 8 cores in the entire group of 16</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true"></a>      <span class="ex">HostBridge</span>  # The video card plugged into the PCI slot directly linked to this core group.</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true"></a>        <span class="ex">PCIBridge</span></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true"></a>          <span class="ex">PCIBridge</span></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true"></a>            <span class="ex">PCI</span> 62:00.0 (VGA)</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true"></a>    <span class="ex">Group0</span> L#1  # Another group of 16 nodes</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true"></a>      <span class="ex">NUMANode</span> L#1 (P#1 126GB)  # <span class="ex">With</span> another NUMA zone attached</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true"></a>        <span class="ex">...</span>  </span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true"></a>      <span class="ex">HostBridge</span>  # This group is linked to a local storage drive: <span class="st">&quot;sda&quot;</span></span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true"></a>        <span class="ex">PCIBridge</span></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true"></a>          <span class="ex">PCI</span> 43:00.0 (SATA)</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true"></a>            <span class="ex">Block</span>(Disk) <span class="st">&quot;sda&quot;</span></span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true"></a>    <span class="ex">Group0</span> L#2</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true"></a>        <span class="ex">...</span></span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true"></a>      <span class="ex">HostBridge</span>  # This group is connected to the high-bandwidth InfiniBand network card</span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true"></a>        <span class="ex">PCIBridge</span></span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true"></a>          <span class="ex">PCI</span> 21:00.0 (InfiniBand)</span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true"></a>            <span class="ex">Net</span> <span class="st">&quot;ibp33s0&quot;</span></span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true"></a>            <span class="ex">OpenFabrics</span> <span class="st">&quot;mlx5_0&quot;</span></span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true"></a>    <span class="ex">Group0</span> L#3</span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true"></a>        <span class="ex">...</span></span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true"></a>  <span class="ex">Package</span> L#1  # This details the hardware associated with the 2nd CPU socket</span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true"></a>    <span class="ex">Group0</span> L#</span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true"></a>        <span class="ex">...</span></span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true"></a>      <span class="ex">HostBridge</span>  # This group is connected to the ethernet network card</span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true"></a>        <span class="ex">PCIBridge</span></span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true"></a>          <span class="ex">PCI</span> e1:00.0 (Ethernet)</span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true"></a>            <span class="ex">Net</span> <span class="st">&quot;em1&quot;</span></span>
<span id="cb2-47"><a href="#cb2-47" aria-hidden="true"></a>    <span class="ex">Group0</span> L#5</span>
<span id="cb2-48"><a href="#cb2-48" aria-hidden="true"></a>        <span class="ex">...</span></span>
<span id="cb2-49"><a href="#cb2-49" aria-hidden="true"></a>    <span class="ex">Group0</span> L#6</span>
<span id="cb2-50"><a href="#cb2-50" aria-hidden="true"></a>        <span class="ex">...</span></span>
<span id="cb2-51"><a href="#cb2-51" aria-hidden="true"></a>    <span class="ex">Group0</span> L#7</span>
<span id="cb2-52"><a href="#cb2-52" aria-hidden="true"></a>        <span class="ex">...</span></span></code></pre></div>
<h2 id="exploring-slurm-pinning-options-with-taskset">Exploring Slurm pinning options with <code>taskset</code></h2>
<p>The tool <code>taskset</code> provides information on the pinning settings currently applied to a running process.</p>
<p>Here’s a Slurm script that we can edit and submit to explore the effects of different Slurm options on pinning:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true"></a><span class="co">#!/usr/bin/env bash</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true"></a><span class="co">#SBATCH --nodes=1</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true"></a><span class="co">#SBATCH --ntasks-per-node=8</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true"></a><span class="co">#SBATCH --exclusive</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true"></a><span class="co">#SBATCH -p cosma8</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true"></a><span class="co">#SBATCH -t 00:10:00</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true"></a><span class="co">#SBATCH -A &lt;PROJECT_CODE&gt;</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true"></a></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true"></a><span class="ex">srun</span> bash -c <span class="st">&#39;echo -n &quot;task ${SLURM_PROCID} (node ${SLURM_NODEID}): &quot;; taskset --cpu-list --pid ${BASHPID}&#39;</span> <span class="kw">|</span> <span class="fu">sort</span></span></code></pre></div>
<p>Again, copying this into a script called something like <code>print_pinning_info.sh</code> and submitting it with <code>sbatch print_pinning_info.sh</code> will produce an output file called something like <code>slurm-808080.out</code> with the contents:</p>
<pre><code>task 0 (node 0): pid 2752726&#39;s current affinity list: 0-255
task 1 (node 0): pid 2752727&#39;s current affinity list: 0-255
task 2 (node 0): pid 2752728&#39;s current affinity list: 0-255
task 3 (node 0): pid 2752729&#39;s current affinity list: 0-255
task 4 (node 0): pid 2752730&#39;s current affinity list: 0-255
task 5 (node 0): pid 2752731&#39;s current affinity list: 0-255
task 6 (node 0): pid 2752732&#39;s current affinity list: 0-255
task 7 (node 0): pid 2752733&#39;s current affinity list: 0-255</code></pre>
<p>In the submission script we requested 8 processes<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>, which have been assigned, however <code>taskset</code> reports that the current affinity is set to <code>0-255</code> on every process, that is these processes are allowed on any logical core with an index between 0 and 255. Since the entire system has a total of 256 logical cores, this means pinning is <em>not</em> enabled and the system is free to migrate these processes between any of its available cores.</p>
<h3 id="using---cpu-bindcores">Using <code>--cpu-bind=cores</code></h3>
<p><strong>Edit the submission script to add the flag <code>--cpu-bind=cores</code> to the <code>srun</code> command.</strong> The line containing <code>srun</code> should now look like:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true"></a><span class="ex">srun</span> --cpu-bind=cores bash ...</span></code></pre></div>
<p>Submitting again should produce output like:</p>
<pre><code>task 0 (node 0): pid 2753061&#39;s current affinity list: 0,128
task 1 (node 0): pid 2753062&#39;s current affinity list: 64,192
task 2 (node 0): pid 2753063&#39;s current affinity list: 1,129
task 3 (node 0): pid 2753064&#39;s current affinity list: 65,193
task 4 (node 0): pid 2753065&#39;s current affinity list: 2,130
task 5 (node 0): pid 2753066&#39;s current affinity list: 66,194
task 6 (node 0): pid 2753067&#39;s current affinity list: 3,131
task 7 (node 0): pid 2753068&#39;s current affinity list: 67,195</code></pre>
<p>Notice that each process now has a very constrained affinity list, for example process 0 can only be assigned to core 0 or 128, i.e. the two logical cores associated with the physical core 0. It might seem odd that process 0 is assigned to core 0, and then process 1 is assigned to core 64. This is a consequence of the way Slurm initially distributes processes across cores. On this system, Slurm is trying to distribute processes so that they are balanced between the two CPU sockets. We’ll explore process or thread distribution in a later section.</p>
<h3 id="other-pinning-options">Other pinning options</h3>
<p><a href="https://slurm.schedmd.com/mc_support.html">The Slurm documentation</a> contains detailed information about more flags to control pinning, including further options for the <code>--cpu-bind</code> flag.</p>
<h2 id="pinning-threads-with-openmp">Pinning threads with OpenMP</h2>
<p>Previously, we’ve used <code>srun</code> to pin MPI processes, but in hybrid MPI-OpenMP codes it can improve performance to also pin OpenMP threads. We can ensure these threads are pinned to OpenMP <em>places</em> by setting the environment variable <code>OMP_PROC_BIND</code><a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> to <code>true</code>. The possible places in the OpenMP standard<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> are <code>threads</code>, <code>cores</code> and <code>sockets</code>, all set in the environment variable <code>OMP_PLACES</code>. As a starting point, we recommend putting the following settings in your Slurm submission scripts, and then experimenting with the other options:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true"></a><span class="bu">export</span> <span class="va">OMP_PROC_BIND=</span>true</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true"></a><span class="bu">export</span> <span class="va">OMP_PLACES=</span>cores</span></code></pre></div>
<h2 id="thread-distribution">Thread distribution</h2>
<p>While pinning threads ensures threads stick to the cores they’re initially assigned, we can also control the way in which threads are initially distributed with <em>thread distribution</em>. By combining these techniques, we can have extremely precise control over which threads run on which cores in a system, and potentially unlock performance gains as a result.</p>
<p><em>Thread distribution</em> is the way in which threads or processes are initially distributed amongst cores and sockets. This is generally only important when threads access resources, like the filesystem, GPUs, or memory, in an unbalanced way. For example, a multi-GPU simulation may sensibly assign one thread per GPU to process data transfers. It’s more performant to ensure these specific threads are spread across cores in such a way that any data transfers are also spread across different transfer buses. This helps maximise the available bandwidth.</p>
<p>Let’s look at some specific hardware. In our earlier walk-through of the <code>hwloc-ls</code> output, we found the InfiniBand network card attached to group L#2. Any cores within this group will have the fastest access to this card. If a piece of software is written in such a way that thread or process 0 is the process that communicates most with other nodes, it may improve performance to ensure that thread is initially distributed (and pinned) to group L#2, i.e. cores between 32-48.</p>
<p>A more advanced example involving GPUs is given in <a href="https://docs.lumi-supercomputer.eu/runjobs/scheduled-jobs/distribution-binding/#gpu-binding">LUMI’s documentation</a>, where users are advised to assign MPI ranks associated with certain GPUs to the NUMA zones that are directly linked to each GPU. In this case, MPI processes must be assigned to very specific CPU cores with the Slurm option <code>--cpu-bind=map_cpu:&lt;map&gt;</code>.</p>
<h2 id="other-resources">Other resources</h2>
<ul>
<li><a href="https://gcc.gnu.org/onlinedocs/libgomp/GOMP_005fCPU_005fAFFINITY.html">OpenMP CPU affinity for GCC’s libgomp</a></li>
<li><a href="https://lumi-supercomputer.github.io/LUMI-training-materials/2day-20240502/07_Binding/">LUMI resources on Process and Thread Distribution and Binding</a></li>
<li><a href="https://docs.lumi-supercomputer.eu/runjobs/scheduled-jobs/distribution-binding/">LUMI documentation on distribution and binding</a></li>
<li><a href="https://hpc-docs.uni.lu/jobs/affinity_and_pinning/">ULHPC documentation on affinity and pinning</a></li>
</ul>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>Confusingly, there is not a consistent term to describe thread pinning. You may see any combination of thread, process and CPU along with binding, affinity and pinning.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p>On HPC systems, CPUs, cores, GPUs and most other pieces of hardware are usually indexed from 0. So a two-CPU system would likely report CPU 0 and CPU 1.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3" role="doc-endnote"><p>In the Slurm submission script you will need to replace the text <code>&lt;PROJECT_CODE&gt;</code> with a real project code, which you require to gain access to any DiRAC system, and can usually find through the SAFE system.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4" role="doc-endnote"><p>For simplicity, we will use tasks and processes interchangeably.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5" role="doc-endnote"><p>See the <a href="https://www.openmp.org/spec-html/5.0/openmpse52.html#x291-20580006.4">OMP_PROC_BIND documentation</a> for information on the more advanced pinning settings: <code>master</code>, <code>close</code>, and <code>spread</code>, all options that control the distribution of threads within places.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6" role="doc-endnote"><p>Some implementations of OpenMP offer more sophisticated options for <code>OMP_PLACES</code>, like pinning to NUMA zones. You may wish to experiment with these options for extra performance gains.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
